# üß† One-Hot Encoding (NLP)

## üìå Definition
- A basic text representation technique in NLP
- Converts each word into a binary vector
- Vector length equals the vocabulary size
- Exactly one index is `1`, all others are `0`

---

## üìù Example

Sentence:
I love NLP

Vocabulary:
["I", "love", "NLP"]


One-Hot Vectors:
- I ‚Üí [1, 0, 0]
- love ‚Üí [0, 1, 0]
- NLP ‚Üí [0, 0, 1]

---
# Bag of Words (BoW) ‚Äì NLP Notes

## What is Bag of Words?
Bag of Words (BoW) is a text representation technique that converts text into numerical vectors based on word frequency, ignoring grammar and word order.

---

## Core Idea
- Treat text as a **bag of words**
- Order of words is ignored
- Only **frequency/count** of words matters

---

## Steps Involved
1. Collect text documents
2. Tokenize text
3. (Optional) Remove stopwords & punctuation
4. Create a vocabulary of unique words
5. Count word occurrences
6. Represent each document as a vector

---

## Example
Documents:
- D1: "I love NLP"
- D2: "I love AI"

Vocabulary:
["I", "love", "NLP", "AI"]


Vectors:
D1 ‚Üí [1, 1, 1, 0]
D2 ‚Üí [1, 1, 0, 1]


---

## Document-Term Matrix (DTM)
- Rows ‚Üí Documents  
- Columns ‚Üí Words  
- Values ‚Üí Frequency  

---

## Types of BoW
- **Binary BoW** ‚Üí 0 or 1 (presence)
- **Count BoW** ‚Üí word frequency
- **Term Frequency (TF)** ‚Üí normalized counts

---

## Advantages
- Simple and easy to implement
- Fast computation
- Works well as a baseline model

---

## Disadvantages
- Ignores word order
- No semantic meaning
- Produces sparse vectors
- Large vocabulary size

---

## Use Cases
- Text classification
- Spam detection
- Basic sentiment analysis
- Baseline NLP models

---

## Key Limitation
BoW cannot capture context, meaning, or relationships between words.

---
# N-grams 

## What is an N-gram?
An N-gram is a sequence of **N consecutive words or characters** extracted from a text.  
It helps capture **local word order**, which Bag of Words ignores.

---

## Types of N-grams
- **Unigram (1-gram)** ‚Üí single word  
- **Bigram (2-gram)** ‚Üí two consecutive words  
- **Trigram (3-gram)** ‚Üí three consecutive words  

Example sentence:
"I love NLP"

- Unigrams ‚Üí `["I", "love", "NLP"]`
- Bigrams ‚Üí `["I love", "love NLP"]`
- Trigrams ‚Üí `["I love NLP"]`

---

## Why Use N-grams?
- Capture word order
- Preserve local context
- Differentiate phrases like `"not good"` vs `"good"`

---

## N-grams with Bag of Words
- N-grams extend Bag of Words by using word sequences instead of single words
- Commonly used with **CountVectorizer** and **TF-IDF**

---

## ngram_range (sklearn)
- `(1,1)` ‚Üí Unigrams
- `(2,2)` ‚Üí Bigrams
- `(1,2)` ‚Üí Unigrams + Bigrams
- `(1,3)` ‚Üí Uni + Bi + Trigrams

---

## Advantages
- Better context than BoW
- Simple to implement
- Improves text classification performance

---

## Disadvantages
- Large vocabulary size
- Sparse feature vectors
- Still no semantic understanding

---

## Use Cases
- Sentiment analysis
- Text classification
- Basic language modeling
- Phrase detection

---

## Key Takeaway
N-grams capture **local context and word order**, making them more powerful than simple Bag of Words.

---
# TF-IDF

## What is TF-IDF?
TF-IDF stands for **Term Frequency ‚Äì Inverse Document Frequency**.  
It is a text representation technique that measures **how important a word is to a document relative to a collection of documents**.

---

## Why TF-IDF?
- Common words like *‚Äúis‚Äù, ‚Äúthe‚Äù, ‚Äúand‚Äù* appear frequently but carry little meaning
- TF-IDF **reduces the weight of common words**
- Highlights **important and rare words**

---

## Components

### 1. Term Frequency (TF)
Measures how often a word appears in a document.
TF(word) = (Number of times word appears in document) / (Total words in document)


---

### 2. Inverse Document Frequency (IDF)
Measures how rare a word is across all documents.


IDF(word) = log(Total documents / Documents containing the word)


---

### 3. TF-IDF Formula


TF-IDF = TF √ó IDF


---

## Intuition
- Word appears **many times in one document** ‚Üí important
- Word appears **in many documents** ‚Üí less important
- TF-IDF balances both

---

## Example
Documents:
- D1: "I love NLP"
- D2: "I love AI"

- Word **"love"** appears in both ‚Üí low IDF
- Word **"NLP"** appears only in D1 ‚Üí high IDF

‚úî TF-IDF gives **higher weight to "NLP"**

---

## TF-IDF vs Bag of Words
| Feature | BoW | TF-IDF |
|------|-----|--------|
| Uses frequency | ‚úÖ | ‚úÖ |
| Penalizes common words | ‚ùå | ‚úÖ |
| Importance-based | ‚ùå | ‚úÖ |
| Sparse vectors | ‚úÖ | ‚úÖ |

---

## Advantages
- Reduces impact of stopwords
- Better feature importance than BoW
- Simple and effective
- Widely used baseline

---

## Disadvantages
- Ignores word order
- No semantic meaning
- Still produces sparse vectors

---

## Use Cases
- Text classification
- Search engines
- Information retrieval
- Sentiment analysis

---

## Key Takeaway
TF-IDF highlights **important words** by down-weighting common ones, making it more powerful than simple Bag of Words.

---